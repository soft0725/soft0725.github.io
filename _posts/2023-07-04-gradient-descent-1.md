---
title: 경사 하강법 (쉽게 알아보자)
date: 2023-07-04 12:15:00 +0900
categories: [딥러닝 공부]
tags: [경사 하강법]
---

# 경사 하강법
경사 하강법을 위키백과에서 찾아보면 아래와 같다.

경사 하강법은 1차 근삿값 발견용 최적화 알고리즘이다. 기본 개념은 함수의 기울기를 구하고 경사의 반대 방향으로 계속 이동시켜 극값에 이를 때까지 반복시키는 것이다. [위키백과](https://ko.wikipedia.org/wiki/%EA%B2%BD%EC%82%AC_%ED%95%98%EA%B0%95%EB%B2%95) 

그냥 쉽게 말해서 경사 하강법은 맨 아래로 이동한다고 생각하면 된다. 

MSE를 예로 들어서 그래프를 그리면 평균 제곱 오차를 하기 때문에, 
2차원 그래프로 나타나게 된다.  
그래프의 모양은 아래로 볼록한 모양이 된다. 

![png](/assets/img/2023-07-04/Snipaste_2023-07-04_12-15-42.png)

## MSE를 미분하면? 

경사 하강법은 미분값이 양수 일 때 x - f'(x) > x 조건을 만족하면서 w에 MSE를 미분한 값을 빼주면 된다.  

![png](/assets/img/2023-07-04/Snipaste_2023-07-04_12-19-34.png)

# 경사 하강법을 코드로 구현해보자

그럼 이를 코드로 구현해서 쉽게 알아보자 

```python
# x, y의 데이터
x_data = [1.0, 2.0, 3.0] 
y_data = [2.0, 4.0, 6.0]

# W 가중치의 값 
w = 3.0 # 어디서나 시작해도 최솟값은 같기에 w값은 상관없다.

# Loss : SE  
def loss(x, y):
    y_pred = x * w
    return (y_pred - y)**2 

# SE를 미분
def gradient(x, y):
    return 2*x*(w*x - y)
    
loss_sum = 0

for x, y in zip(x_data, y_data):
    loss_sum = loss(x, y)

print(f'before loss value : {loss_sum/len(x_data)}')
```
출력값 : before loss value : 3.0
처음 가설을 정의하고 최적화를 시키지 않았을 때는 3.0이라는 오차값이 나온다.  

## 처음 가설을 그래프로 
```python
import matplotlib.pyplot as plt

# 예측값 계산
y_pred = [w * x for x in x_data]

# 그래프 그리기
plt.scatter(x_data, y_data, color='red')
plt.plot(x_data, y_pred, color='blue')
plt.show()
```
![png](/assets/img/2023-07-04/Snipaste_2023-07-04_12-18-21.png)

그래프를 보면 빨간점을 실제 데이터라고 했을 때, 파란색 가설이 많이 멀어지는걸 볼 수 있다.  


## 최적화 시켜보자 

```python
# draw line 

for epoch in range(100):
    for x_val, y_val in zip(x_data, y_data):
        grad = gradient(x_val, y_val)
        w = w - 0.01 * grad #  Learning rate 0.01
        print(f"\tgrad :", x_val, y_val, grad)
        l = loss(x_val, y_val)
        
print("progress :", epoch, 'w =', round(w, 4), 'loss =', round(l, 4), '\n')
```
위 코드는 W 를 최적화 시켜준다.  

원리를 알아보자 

epoch은 얼마나 반복을 할지 정해주는 변수이다.  
만약 epoch가 100이면 100번만 반복하고 끝낸다.

그리고 x_data, y_data를 꺼내서 하나씩 비교를 하는데 총 3번 비교를 하고 gradient함수의 return 2*x*(w*x - y)을 통해서 se를 미분한 값을 리턴한다.  

그리고 경사 하강법은 w에서 mse를 미분한 값을 빼주어야 하기 때문에 w - w x 0.01 을 해주고 0.01은 step size로 조금 심화편에서 다루도록 하겠다.  

이제 마지막으로 l이라는 변수를 통해서 현재 loss값을 확인하면 

```md
grad : 1.0 2.0 2.0
	grad : 2.0 4.0 7.84
	grad : 3.0 6.0 16.228800000000003
progress : 0 w = 2.7393 loss = 4.9192 

	grad : 1.0 2.0 1.478624
	grad : 2.0 4.0 5.796206080000001
	grad : 3.0 6.0 11.998146585600002
progress : 1 w = 2.5466 loss = 2.6888 

	grad : 1.0 2.0 1.093164466688
	grad : 2.0 4.0 4.285204709416959
	grad : 3.0 6.0 8.870373748493105
progress : 2 w = 2.4041 loss = 1.4696 

	grad : 1.0 2.0 0.808189608196038
	grad : 2.0 4.0 3.1681032641284688
	grad : 3.0 6.0 6.557973756745934
progress : 3 w = 2.2988 loss = 0.8033 

	grad : 1.0 2.0 0.5975042756146296
	grad : 2.0 4.0 2.3422167604093467
	grad : 3.0 6.0 4.8483886940473475
progress : 4 w = 2.2209 loss = 0.4391 

	grad : 1.0 2.0 0.4417420810132029
	grad : 2.0 4.0 1.731628957571754
	grad : 3.0 6.0 3.5844719421735274
progress : 5 w = 2.1633 loss = 0.24 

	grad : 1.0 2.0 0.3265852213980329
	grad : 2.0 4.0 1.2802140678802907
	grad : 3.0 6.0 2.6500431205121995
progress : 6 w = 2.1207 loss = 0.1312 

	grad : 1.0 2.0 0.241448373202223
	grad : 2.0 4.0 0.9464776229527132
	grad : 3.0 6.0 1.9592086795121144
progress : 7 w = 2.0893 loss = 0.0717 

	grad : 1.0 2.0 0.17850567968888154
	grad : 2.0 4.0 0.699742264380415
	grad : 3.0 6.0 1.44846648726746
progress : 8 w = 2.066 loss = 0.0392 

	grad : 1.0 2.0 0.13197139106214628
	grad : 2.0 4.0 0.5173278529636143
	grad : 3.0 6.0 1.070868655634678
progress : 9 w = 2.0488 loss = 0.0214 

	grad : 1.0 2.0 0.09756803306893769
	grad : 2.0 4.0 0.38246668963023467
	grad : 3.0 6.0 0.7917060475345838
progress : 10 w = 2.0361 loss = 0.0117 

	grad : 1.0 2.0 0.07213321766426173
	grad : 2.0 4.0 0.28276221324390605
	grad : 3.0 6.0 0.5853177814148793
progress : 11 w = 2.0267 loss = 0.0064 

	grad : 1.0 2.0 0.05332895341780031
	grad : 2.0 4.0 0.20904949739777834
	grad : 3.0 6.0 0.4327324596134048
progress : 12 w = 2.0197 loss = 0.0035 

	grad : 1.0 2.0 0.03942673520922124
	grad : 2.0 4.0 0.15455280202014876
	grad : 3.0 6.0 0.3199243001817109
progress : 13 w = 2.0146 loss = 0.0019 

	grad : 1.0 2.0 0.029148658460999677
	grad : 2.0 4.0 0.1142627411671171
	grad : 3.0 6.0 0.23652387421593346
progress : 14 w = 2.0108 loss = 0.001 

	grad : 1.0 2.0 0.02154995298411766
	grad : 2.0 4.0 0.08447581569773988
	grad : 3.0 6.0 0.17486493849431994
progress : 15 w = 2.008 loss = 0.0006 

	grad : 1.0 2.0 0.015932138840593524
	grad : 2.0 4.0 0.06245398425512505
	grad : 3.0 6.0 0.12927974740810555
progress : 16 w = 2.0059 loss = 0.0003 

	grad : 1.0 2.0 0.011778821430517006
	grad : 2.0 4.0 0.0461729800076256
	grad : 3.0 6.0 0.09557806861577944
progress : 17 w = 2.0044 loss = 0.0002 

	grad : 1.0 2.0 0.00870822402943805
	grad : 2.0 4.0 0.034136238195397794
	grad : 3.0 6.0 0.0706620130644744
progress : 18 w = 2.0032 loss = 0.0001 

	grad : 1.0 2.0 0.0064380945236521825
	grad : 2.0 4.0 0.02523733053271826
	grad : 3.0 6.0 0.052241274202728505
progress : 19 w = 2.0024 loss = 0.0001 

	grad : 1.0 2.0 0.004759760538470381
	grad : 2.0 4.0 0.01865826131080439
	grad : 3.0 6.0 0.03862260091336189
progress : 20 w = 2.0018 loss = 0.0 

	grad : 1.0 2.0 0.0035189480832178432
	grad : 2.0 4.0 0.013794276486212453
	grad : 3.0 6.0 0.028554152326460525
progress : 21 w = 2.0013 loss = 0.0
```

이런식으로 loss값이 0.0에 점점 가까워진다.   
또한 w값은 어디에서 시작하더라도 최적화가 되기 때문에 w값은 아무리 커도 상관이 없다.  

## 최적화된 W로 가설을 그려보자 

```python
import matplotlib.pyplot as plt

# 예측값 계산
y_pred = [w * x for x in x_data]

# 그래프 그리기
plt.scatter(x_data, y_data, color='red')
plt.plot(x_data, y_pred, color='blue')
plt.show()
```
이제 위에서 w를 최적화 시켰기 때문에 w를 사용해서 그래프를 그려보겠다. 

![png](/assets/img/2023-07-04/Snipaste_2023-07-04_12-17-22.png)

진짜 미쳤다.   
딥러닝 원리가 생각보다 재미없었는데 이거때문에 너무 재밌어졌다.
다음편은 살짝 심화버전으로 올리도록 하겠다.