---
title: 모딥 시즌 1 - ML lec 03
date: '2023-07-24 03:10:00 +0900'
categories: [2. Machine Learning & DeepLearning, 2 - 1 모두를 위한 딥러닝 시즌 1 정리]
tags: [모두를 위한 딥러닝 시즌 1]
math: true
---

# lec 03 정리 

lec 02 부분에서 이어집니다.

## How to minimize cost?  
어떻게 cost를 최소화시키는지 알아보도록하자.  
전 블로그 lec 02에서 우리의 가설을 아래와 같이 정의하였다.  
$$H(x) = Wx + b$$   

그리고 가설 $H(x)$를 cost function에 대입하면 아래와 같다.  

$$cost(W, b) = \frac{1}{n} \sum_{i=1}^{n}(H(x^{(i)}) - y^{(i)})^2$$  

그럼 위 $cost$를 $minimize$ 하는 $W, b$ 값을 어떻게 찾을 수 있을까?   

<br>


`일단` 그 전에 식을 간단하게 바꿔주자  
$H(x) = Wx + b$ 를 아래와 같이 b가 빠진 식으로 바꾸어 주겠다.  
$$H(x) = Wx$$ 

그러면 $cost$는 $W$에 대해서만 나타낼 수 있다.  

$$cost(W) = \frac{1}{n} \sum_{i=1}^{n}(Wx^{(i)} - y^{(i)})^2$$  

<br>

### 한 번 계산해보자  
W = 1, 0, 2 일때 cost(W)의 값을 구해보자. 


$$cost(W) = \frac{1}{n} \sum_{i=1}^{n}(Wx^{(i)} - y^{(i)})^2$$  

데이터는 아래와 같다고 가정한다.  

|X|Y|               
|:---:|:---:|
|1|1|
|2|2|
|3|3|

<br>

1. $W = 1$일 경우의 $cost(W) = ?$

    $$H(x) = Wx$$

    $$\frac{(1*1 - 1)^2 + (1*2 - 2)^2 + (1*3 - 3)^2}{3} = \frac{0}{3} = 0$$

    `Cost(W) = 0`

<br>

2. $W = 3$일 경우의 $cost(W) = ?$

    $$H(x) = Wx$$

    $$\frac{(3*0 - 1)^2 + (3*0 - 2)^2 + (3*0 - 3)^2}{3} = 4.6666...$$

    `Cost(W) = 4.67`

<br>

3. $W = 2$일 경우의 $cost(W) = ?$

    $$H(x) = Wx$$

    $$\frac{(2*1 - 1)^2 + (2*2 - 2)^2 + (2*3 - 3)^2}{3} = 4.6666...$$

    `Cost(W) = 4.67`

위 처럼 나오게 된다.  
    
정리하면  
- W = 1 -> 0 
- W = 0 -> 4.67
- W = 2 -> 4.67 

뭔가 하나의 패턴이 보인다.  
$Cost$의 값이 가장 작을 때의 $W$값이 1, 그 다음 $W = 0, 2$일 때는 조금 값이 커졌다.  
그런데 왜 $W = 0, 2$일 때 cost값은 같을까?  

위 $cost(W)$를 그림으로 그려보겠다.  

<br>

### cost 시각화

$$cost(W) = \frac{1}{n} \sum_{i=1}^{n}(Wx^{(i)} - y^{(i)})^2$$  

일단 위 식을 보면 가장 먼저 2차 방정식인 것을 알 수 있다.  
그러면 결국 아래로 볼록한 그래프가 만들어지게 된다.  

<img src="/assets/img/Modeep1/gradient descent.png" alt="." width="40%" height="40%">  

여기서 목표는 $cost$를 작게 만드는 $W$를 찾는다는 것을 잊지 말자.  
그럼 저걸 어떻게 찾아야할까?  

<br><br>

## Gradient descent algorithm
Gradient descent algorithm은 경사 하강법이라고 불린다.  
위에서 $cost$를 작게 만드는 $W$를 어떻게 찾을지가 문제였는데  
그 문제는 이 친구가 해준다.  

원리는 정말 간단하다 `미분`을 사용하는 것이다.  
쉽게 말하자면 아무값 $W$가 경사를 타면서 내려온다고 생각하면 된다.
그걸 시각화하면 아래와 같다.  

<img src="/assets/img/Modeep1/gradient descent step.png" alt="." width="40%" height="40%">

그럼 식은 어떻게 될까?  

$$W = W - α\frac{∂}{∂W}cost(W)$$
$W$는 현재 값 $W$에서 cost를 미분한 값에 α를 곱한 값을 받는다.  
그러면서 새로운 $W$는 계속 업데이트가 되면서 미분값이 음수일 때는 
+를 하여 오른쪽으로, -일때는 왼쪽으로 움직이게 된다.  
이때 α라는 것은 learning rate, 학습률 이라고 하며 기본적으로 0.01, 0.001과 같은 숫자를 가진다.  


<br>

그럼 위 식을 가설에 대입해서 계산해보자  
그 전에 계산이 편리하게 하기 위해 분모에 2를 추가해주었다.  

$$cost(W) = \frac{1}{2n} \sum_{i=1}^{n}(Wx^{(i)} - y^{(i)})^2$$  

$$\frac{1}{2n} \sum_{i=1}^{n} 2(Wx^{(i)} - y^{(i)})x^{(i)}$$  

$$\frac{1}{n} \sum_{i=1}^{n} (Wx^{(i)} - y^{(i)})x^{(i)}$$  

결국엔 마지막으로 아래와 같이 된다.  

$$\frac{1}{n} \sum_{i=1}^{n} (Wx^{(i)} - y^{(i)})x^{(i)}$$  

그럼 이제 이 식을 가지고 경사 하강법에 적용시켜보도록 하겠다. 

적용을 시켜보면 아래와 같다. 
$$W = W - α\frac{1}{n} \sum_{i=1}^{n} (Wx^{(i)} - y^{(i)})x^{(i)}$$  

정말 간단하다.  
이제 위 식을 그대로 코드로 구현을 하기만 한다면 $cost$를 최소화 시키는 $W$의 값을 찾을 수 있다.
 
<br>

### 주의점!! 
저렇게 코드로만 구현해서 끝나는 것이 아니다.  

<img src="/assets/img/Modeep1/Convex function.png" alt="." width="40%" height="40%">

$Cost function, W, b$ 를 그래프로 그리게 되면 3차원으로 위와 같은 모양이 나온다.  
그런데 그렇게 되면 보이는 것 처럼 2가지의 길 중 어느 한쪽으로 빠지게 되는데 저렇게 되면 최적화가 잘 되지 않아 동작을 잘 하지 못한다.   

따라서 위의 모양이 아닌   
<img src="/assets/img/Modeep1/Convex function good.png" alt="." width="40%" height="40%">

이 모양인지 확인을 먼저 해보자  
그래야 어떤 길로 가던지 간에 가장 낮은 지역을 찾기 때문이다.