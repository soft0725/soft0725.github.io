---
title: 모딥 시즌 1 - ML lec 06-2
date: '2023-07-25 20:00:00 +0900'
categories: [DeepLearning, 모두를 위한 딥러닝 시즌 1]
tags: [모두를 위한 딥러닝 시즌 1]
math: true
---

# lec 06 정리
lec 06-1 와 이어집니다.  

<br>

## lec 06-1 에서는

여러 개를 분류하는 방법으로 binary classification 방법을 사용해서 Multimial classification을 할 수 있다고 했다.  

방법은 아래와 같은 식을 총 3개를 쓰는 것인데

<img src="/assets/img/Modeep1/simple sigmoid step.png" width="50%" height="50%">

$$\begin{pmatrix}w_{11}&w_{12}&w_{13} \end{pmatrix}$$ $$\begin{pmatrix}x_1\\x_2\\x_3 \end{pmatrix}$$ = $$\begin{pmatrix}w_{11}x_{1} +&w_{12}x_{2} + &w_{13}x_{3} \end{pmatrix}$$  

$$\begin{pmatrix}w_{11}&w_{12}&w_{13} \end{pmatrix}$$ $$\begin{pmatrix}x_1\\x_2\\x_3 \end{pmatrix}$$ = $$\begin{pmatrix}w_{11}x_{1} +&w_{12}x_{2} + &w_{13}x_{3} \end{pmatrix}$$  

$$\begin{pmatrix}w_{11}&w_{12}&w_{13} \end{pmatrix}$$ $$\begin{pmatrix}x_1\\x_2\\x_3 \end{pmatrix}$$ = $$\begin{pmatrix}w_{11}x_{1} +&w_{12}x_{2} + &w_{13}x_{3} \end{pmatrix}$$  

이렇게 되면 너무 복잡하고 비효율적이니 행렬의 형태로 정리해서 

$$\begin{pmatrix}w_{A1}&w_{A2}&w_{A3}\\w_{B1}&w_{B2}&w_{B3}\\w_{C1}&w_{C2}&w_{C3} \end{pmatrix}$$ $$\begin{pmatrix}x_1\\x_2\\x_3 \end{pmatrix}$$ = $$\begin{pmatrix}w_{A1}x_{1} +&w_{A2}x_{2} + &w_{A3}x_{3}\\w_{B1}x_{1} + &w_{B2}x_{2} + &w_{B3}x_{3}\\w_{C1}x_{1} + &w_{C2}x_{2} + &w_{C3}x_{3} \end{pmatrix}$$  = $$\begin{pmatrix}\hat{Y}_A\\ \hat{Y}_B\\ \hat{Y}_C \end{pmatrix}$$

이런식으로 정리할 수 있다까지만 정리하였다.  

<br>

## Where is Sigmoid? 
자 그럼 마지막으로 저 식이 어떤 형태로 나오는지를 결정해줘야 한다.  
위 식을 보면 그 어디에도 Sigmoid를 사용하는 식이 보이지 않는다.  

$$H(x) = WX$$

단순히 $WX$를 곱한 식 밖에 보이지 않는다. 
만약 위 식이 끝이라면 

$$H(x) = WX = \hat{Y}_{A}$$ 

와 같은 식이 되어버린다.  

그럼 어떻게 해야할까?  

<br><br>

## Softmax 

이럴때 Softmax를 사용할 수 있다.  
Softmax는 위 $Y$값의 출력을 총합 1.0으로 만들어준다.  

- 예를 들어보자.  

만약 Y의 출력값이 아래와 같다면 

$$\begin{bmatrix}2.0\\1.0\\0.1 \end{bmatrix}$$ 

위 식을 

$$\begin{bmatrix}0.7\\0.2\\0.1 \end{bmatrix}$$ 

과 같은 형태로 총 합이 1.0이 되도록 만들어준다.  

<br>

## 순서는 어떻게?

가장 먼저 출력값을 Softmax 함수를 통해서 1과 0 사이의 값으로 바꾸어준다.  

<img src="/assets/img/Modeep1/softmax step.png" width="90%" height="90%">

그러면 0.7, 0.2, 0.1과 같은 값으로 나오는데 이 값은 총합이 1이며 마치 확률과 같은 형태의 값을 가진다.  
따라서 가장 큰 값을 1로, 나머지는 다 0으로 처리해버리는 one-hot encoding을 사용할 수 있다.  

그럼 그 다음은 어떻게 해야할까? 

이제 실제값과 예측값이 맞는지 비교하는 cost function을 사용해서 얼마나 모델이 잘 맞추는지 확인 할 수 있다.
이때 사용되는 cost function은 Cross entropy라고 부르는 함수를 사용한다.  

<img src="/assets/img/Modeep1/cross entropy.png" width="90%" height="90%">

<br>

## Cross entropy cost function 

$$D(S, L) = - \sum_{i} L_{i} log(S_{i})$$

식은 위와 같으며 어떤 방식으로 동작할까?  
그 전에 이 부분을 다르게 고쳐보자

$$- \sum_{i} L_{i} log(S_{i})$$ 

$$- \sum_{i} L_{i} log(\hat{y}_{i})$$ 

$$\sum_{i} (L_{i}) * (- log(\hat{y}_{i}))$$ 

이렇게 바꿔줄 수 있다.  
뒤에 보면 익숙한 기호를 볼 수 있는데 $-log(\hat{y})$은 0과 1 사이의 값을 가진다.  
따라서 위 식을 그래프로 그려보면 아래와 같으며 식이 가질 수 있는 범위는 파란색 선이다.

<img src="/assets/img/Modeep1/-log(x) 관심영역.png" width="40%" height="40%">

그럼 값이 0일 때는 무한대에 가까워지고, 1일 때는 0에 가까워진다.  
그걸 활용하여 계산할 수 있다.  

<br>

### Cross entropy 계산 

$$\sum_{i} (L_{i}) * - log(\hat{y}_{i})$$  

식이 이렇게 있다고 할 때, 정답이 $\begin{bmatrix}0\\\1 \end{bmatrix}$ 이며 답은 B 라고 해보자.  

그리고 예측한 값 $\hat{Y}$가 $\begin{bmatrix}0\\\1 \end{bmatrix}$ 과 $\begin{bmatrix}1\\\0 \end{bmatrix}$ 이라면 어떻게 될까? 


<br> 

- 정답이 $\begin{bmatrix}0\\\1 \end{bmatrix}$ 일 때 예측한 값 $\begin{bmatrix}0\\\1 \end{bmatrix}$  

위 식에서 $L_i$가 정답이고, 뒤에 $- log(\hat{y}_{i})$ 이 부분이 예측한 값이다.  
이제 이것을 곱셈을 하게 되는데 처음 앞부분은 그대로 쓰고 $\begin{bmatrix}0\\\1 \end{bmatrix}$ 이때 뒤에는 $-log(\begin{bmatrix}0\\\1 \end{bmatrix})$을 하여  
0일 경우에는 무한대로 가까워지고, 1일 경우에는 0에 가까워진다.  

따라서 서로 곱하면 

$$\begin{bmatrix}0\\1 \end{bmatrix}$$ * $$\begin{bmatrix}0\\1 \end{bmatrix}$$ = $$\begin{bmatrix}0\\1 \end{bmatrix}$$ * $$\begin{bmatrix}∞\\0 \end{bmatrix}$$ = $$\begin{bmatrix}0\\0 \end{bmatrix}$$

이 되고 이것을 시그마에 넣어서 전부 계산하면 0이 된다.

~~곱셈 기호를 못찾겠다.~~

<br><br>

- 반대로 정답이 $\begin{bmatrix}0\\\1 \end{bmatrix}$ 일 때 예측한 값 $\begin{bmatrix}1\\\0 \end{bmatrix}$  

위 식과 똑같은 원리로 계산해보면 1일 경우에는 0에 가까워지고, 1일 경우에는 ∞에 가까워진다.  

따라서 서로 곱하면 

$$\begin{bmatrix}0\\1 \end{bmatrix}$$ * $$\begin{bmatrix}0\\1 \end{bmatrix}$$ = $$\begin{bmatrix}0\\1 \end{bmatrix}$$ * $$\begin{bmatrix}0\\∞ \end{bmatrix}$$ = $$\begin{bmatrix}0\\∞ \end{bmatrix}$$

이 되고 이것을 시그마에 넣어서 전부 계산하면 ∞이 된다.


그러면 $cost$는 값이 작을 때 잘 된것이고, 값이 클 때 잘못된 것이기 때문에 cost가 잘 작동하는 것을 알 수 있다.
그리고 정답이 반대가 되어도 원리는 똑같으니 한 번 따로 계산을 해보길 바란다. :)

<br>

### Logistic cost VS cross entropy 

$$H((x),y) = -ylog(H(x)) - (1 - y)log(1 - H(x))$$ 

$$D(S, L) = - \sum_{i} L_{i} log(S_{i})$$

서로 어떤 차이가 있을까?  

일단 결론만 말하자면 서로 같다.  

처음 $H((x), y)$와 $D(S, L)$ 부분이 앞은 예측값과 뒤는 정답으로 같고 뒤는 내가 알아서 풀어보라고 하셨다.  

솔직히 위 식이 어떻게 같은지는 모르겠지만 이번 글 말고 다른 글에서 한번 풀어보도록 하겠다... 

마지막으로 위 식을 cost function으로 만들어보면 

$$cost = \frac{1}{n} \sum_{i} D(S(Wx + b), L)$$

이렇게 되고 이를 경사 하강법에 적용시키면 된다.  

<br><br>