---
title: 모딥 시즌 1 - ML lec 10 - 2
date: '2023-07-27 19:07:00 +0900'
categories: [DeepLearning, 모두를 위한 딥러닝 시즌 1]
tags: [모두를 위한 딥러닝 시즌 1]
math: true
---

# lec 10 - 2 정리  
앞 글 10-1에서 이어집니다.

## RBM
앞 글에서 Vanishing gradient가 생기는 이유에 대해서 적었다. 
그리고 이번에는 초깃값을 어떻게 설정해야 하는지에 대해서 적어보겠다.    

- 초깃값은 0이 되면 안된다.  
를 명심하자

일단 초깃값을 정하는 문제에서는 RBM이라는 원리를 사용해서 풀 수 있다.  
이것은 요즘에 잘 쓰진 않지만 그대로 알아는 놓자  

일단 RBM의 원리는 아래와 같다.  

<img src="/assets/img/Modeep1/RBM.png" alt=".">
왼쪽이 forward, 오른쪽이 backward이다.  
왼쪽에서 forward한 다음 그림이 오른쪽 그림인데 이제 오른쪽 그림에서 뒤에서 앞으로 전달 될때의 w값은 바뀌지 않는다.  
그리고 이 값이 앞으로 전달되면 input layer에 저렇게 나오는데, 저 값들과 처음 forward의 input layer을 조절하 는 것이 
RBM이다.   
그리고 이것을 또 다른 말로 인코더, 디코더 라고 한다.  

또한 복잡한 모델에서 RBM은 1, 2, 3, 4가 있다면 1, 2만으로 먼저 RBM을 해서 초깃값을 셋팅하고, 2, 3으로 RBM을 셋팅,  
3, 4로 셋팅할 수 있다.  

그러면 초기화는 끝이 나고 모델을 학습시킬 때 학습한다고 하지 않고 fine tuning이라고 부르기도 한다.  
이미 너무 잘 학습되어 있기 때문이다.  

하지만 가장 큰 단점 RBM 어렵다.  
- 그래서 RBM말고 다른걸 사용해도 되며 쉽고 간단하다.  
 
<img src="/assets/img/Modeep1/Xavier.png" alt=".">

이렇게 tensorflow에서는 구현할 수 있다.  


