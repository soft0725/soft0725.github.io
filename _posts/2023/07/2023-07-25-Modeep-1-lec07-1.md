---
title: 모딥 시즌 1 - ML lec 07-1
date: '2023-07-25 22:54:00 +0900'
categories: [DeepLearning, 모두를 위한 딥러닝 시즌 1]
tags: [모두를 위한 딥러닝 시즌 1]
math: true
---

# lec 07 정리

<br>

이번 글에서는 데이터 전처리 방법, learning rate 조절 방법, overfitting 방지하는 방법 에 대해서 정리해보도록 하겠다.  

<br>

## 올바른 Learning rate 조절 법 

Gradient descent Algorithm에서 사용되는 Learning rate는 작은 값을 가진다고 했었다.  
Learning rate는 식에서 $\alpha$ 로 쓰인다.  

$$W = W - α\frac{1}{n} \sum_{i=1}^{n} (Wx^{(i)} - y^{(i)})x^{(i)}$$  

이 식은 cost를 미분해서 $W$를 업데이트 시키는 식이며 중간에 $\alpha$ 가 잘 보이는 것을 확인할 수 있다.  
그럼 Learning rate는 뭘 의미할까?  

$W$가 경사를 타고 내려올 때 한 번에 얼마나 이동할 건지를 의미한다.  
기본적으로 0.01, 0.001과 같은 값을 가지며 숫자가 크다면 한 번에 많은 경사를 타고 내려올거고 
또 너무 작다면 잘 내려오지 못할 것이다.  

<br>

### Learning rate의 숫자가 크다면 

만약 Learning rate의 숫자가 크다면 한 번에 많은 경사를 내려올 것이다.  
그걸 아래 사진으로 예시를 들어보겠다.  

<img src="/assets/img/Modeep1/overshooting.png" width="50%" height="50%">

한 번에 크게 크게 이동하다가 마지막엔 튕겨서저 나가버린다.  
이 증상을 `Over shooting` 이라고 한다.  

이렇게 되면 학습이 잘 되지 않는다.  

그럼 반대로 Learning rate의 숫자가 작다면 어떻게 될까? 


<br>


### Learning rate의 숫자가 작다면 

<img src="/assets/img/Modeep1/minimum learning rate.png" width="50%" height="50%">

만약 숫자가 작다면 경사를 정말 천천히 내려갈 것이다.  
그럼 결국 학습이 되긴 되지만 $cost$를 작게만드는 값 $W$가 최적화가 되지 않아서  
학습이 덜 된다.  
따라서 learning rate가 작을 때는 cost값을 계속 확인해야한다.  

그럼 결국엔 어떻게 정해야 잘 정했다고 소문이날까? 

<br>

### Learning rate 잘 정하는 법 

그런건 존재하지 않는다.  
일단 있다고 해도 정말 간단하다. 

- 처음에는 0.01로 설정하고 값을 조금씩 조정한다.  
- 그러면서 같이 cost 값을 계속 확인한다.  

<br><br>

## 데이터 전처리 

모델을 학습시키다 보면 가끔씩 데이터를 전처리 해주어야되는 일이 생긴다.  
예시로 특성이 두 개인 데이터를 가지고 모델을 학습시킨다고 했을 때 
특성끼리 값 차이가 많이 나지않는다고 가정한다.  

<img src="/assets/img/Modeep1/show gradient descent-1.png" width="30%" height="30%">

그러면 위와 같이 중간이 움푹 파여있는 2차원 형태로 나타낼 수 있다. 
그럼 만약 특성끼리 많은 차이가 나면 어떻게 될까?

<img src="/assets/img/Modeep1/show gradient descent-2.png" width="70%" height="70%">

이렇게 한쪽으로 많이 쏠리는 것을 확인할 수 있다.  
그럼 이걸 어떻게 해결할까? 

<br>

### 데이터 전처리 쏠림 해결 방법 

<img src="/assets/img/Modeep1/data preprocessing.png" width="70%" height="70%">

위에 있는 것 처럼 총 2 가지 방법을 사용할 수 있다.  

첫 번째는 `zero centered data`이며 이것은 데이터가 어디에 있든지 간에 데이터의 중심을  
원점으로 움직여주는 역할을 한다.  

두 번째는 ` normalized data`이고 이것은 데이터를 조금 압축 시켜주는 역할을 한다.  

데이터 전처리가 필요할 떄는 
- Learning rate는 잘 정의한거 같은데 over shooting이 일어날 때 
- 이상한 동작을 보일 때 

처럼 정의할 수 있다.  

<img src="/assets/img/Modeep1/standardization.png" width="50%" height="50%">

하는 방법은 평균과 분산을 가지고 위 식처럼 간단하게 계산하면 된다.  
그리고 이것을 파이썬 코드로 나타내 본다면 아래와 같다.  

```python 
X_std[:, 0] = (X[:, 0] - X[:, 0].mean()) / X[:, 0].std()
```

<br><br>

## Overfitting 
Overfitting은 데이터가 너무 학습 데이터에만 최적화 되어 있다고 생각하면 된다.  
예를 들어서 수능을 보기 전에 문제집을 푸는데 문제집 답만 외우고 시험을 치는 느낌이다.  
따라서 연습때는 정말 좋지만 실전에서는 완전 바보같은 모델이 만들어 진다.  

<img src="/assets/img/Modeep1/overfitting.png" width="70%" height="70%">

위 데이터는 모델이 학습하는 데이터라고 가정하자.  
왼쪽은 Overfitting이 생기지 않은, 오른쪽은 Overfitting이 발생한 이라고 볼 수 있다. 

<br>

### Overfitring 해결 방법 

Overfittingd을 해결하는 방법은  
- Training data를 추가한다. 
- 중복된 피처의 갯수를 줄인다. 
- Regularization을 한다.  

정도가 있다.  
이중에서 Regularization을 조금 파보도록 하겠다. 

<br>

### Regularization
Regularization은 일반화 시킨다라고 말한다.  
일반화를 시키면 오른쪽 위 그림처럼 선이 꾸불꾸불하지 않고 선을 필 수 있도록 해준다.  
따라서 값은 조금 떨어지겠지만 Overfitting을 방지하는 좋은 방법이다.  

식은 아래와 같이 나타낼 수 있다.  

$$cost = \frac{1}{n} \sum_{i} D(S(Wx + b), L)$$

앞 글에서 사용한 cost 뒤에 $λ\sum_{}w^2$ 을 더해서 나타낼 수 있다. 
그럼 아래처럼 된다.

$$cost = \frac{1}{n} \sum_{i} D(S(Wx + b), L) + λ\sum_{}w^2$$

식을 설명하자면 앞에 붙는 λ는 `Regularization strength`라고 부르며  
각 엘리먼트에 대한 제곱을 각각 더해주고 값을 낮춰줌으로써 꾸불함을 조금 피게 만들어 준다.  

만약 위에서 Regularization strength가 0이라면  
- Regularization을 쓰지 않을거라는 뜻으로 해석이 가능하고 

0.001과 같은 숫자면 
- 쓰긴 쓸건데 그렇게 많이는 안할래 같은 뜻으로 쓸 수 있다.  

크면 클 수록 강하게 준다는 말이 된다.  
