---
title: 모딥 시즌 1 - ML lec 10 - 1
date: '2023-07-27 18:30:00 +0900'
categories: [2. Machine Learning & DeepLearning, 2 - 1 모두를 위한 딥러닝 시즌 1 정리]
tags: [모두를 위한 딥러닝 시즌 1]
math: true
---

# lec 10 - 1 정리  

## ReLU 최고!! 
이번 글에서는 왜 요즘 사람들이 Sigmoid를 쓰지 않는지에 대해서 정리해보도록 하겠다.  

정말 간단하다.  

Backward를 할 때 Backpropagation에서 chain rule를 사용했는데 이때 Sigmoid를 사용하였다.  
Sigmoid의 특징은 0과 1 사이의 값을 가진다는 특징이 있었다.  

그런데 chain rule에서 0과 1 사이의 값으로 계속 곱해나가면 어떻게 될까?  
만약 1만 곱해간다면 상관이 없지만 예를 들어 0.5 * 0.5 = 0.25이다.   
이것처럼 소숫점 자리인 숫자들을 계속 곱해나가면 0.0001 * 0.00001 이런식으로 가다가 결국에는 0이 되어버린다.  
그래서 마지막에는 값이 사라지게 되는데 이때 이것을 `Vanishing gradient`이라고 부른다.  

그럼 이것을 어떻게 해결할 수 있을까?  

`ReLU`라는 다른 활성화 함수를 사용하는 것이다.  
ReLU는 0보다 작다면 그냥 0의 값을 가지고 1보다 크다면 자기 자신의 값을 그대로 가진다.  

그리고 ReLU 말고도 다른 활성화 함수들이 많은데 이것은 다른 카테고리 `정리 & 구현`에 올려놓도록 하겠다.  


