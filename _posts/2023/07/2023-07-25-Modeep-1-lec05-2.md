---
title: 모딥 시즌 1 - ML lec 05-2
date: '2023-07-25 00:14:00 +0900'
categories: [DeepLearning, 모두를 위한 딥러닝 시즌 1]
tags: [모두를 위한 딥러닝 시즌 1]
math: true
---

# lec 05 정리

이번 글은 저번 lec05-1 파트와 이어집니다.
저번 글에서 가설은 Logistic을 사용한다 라는 것까지만 정리하고 끝났는데, 이번 글에서는 어떻게 Cost를 minimize하는지  
정리해보도록 하겠습니다. 

## Cost 
저번 글에서의 가설을 cost에 대입하면 아래와 같고

$$H(x) = Wx + b$$ 

$$cost(W, b) = \frac{1}{n} \sum_{i=1}^{n} (H(x^{(i)}) - y^{(i)})$$

cost를 그려보면 아래와 같았다.

<img src="/assets/img/Modeep1/gradient descent.png" width="50%" height="50%"> 

아래로 매끄럽게 이어지는 아래로 볼록한 모양이였다.  
하지만 이번 Sigmoid를 가설로 정의하였을 때는 어떨까

$$H(X) = \frac{1}{1 + e^{-W^{T }X}}$$

$$cost(W, b) = \frac{1}{n} \sum_{i=1}^{n} (H(x^{(i)}) - y^{(i)})$$

<img src="/assets/img/Modeep1/sigmoid graph.png" width="50%" height="50%"> 

일단 위 그림을 보면 정말 이상하다는 것을 알 수 있다.  
그 이유는 Simgoid가 S자 형태의 모양을 가지는데 이를 제곱했을 때 꾸불한 형태가 남아있기 때문이다.  

이렇게 되면 큰 문제점이 생기는데 그 문제점을 알아보자 

<br>

### Sigmoid를 사용한 cost의 문제점 

그럼 문제점이 도대체 뭘까?

<img src="/assets/img/Modeep1/localminimum.png" width="70%" height="70%">

위 그림을 보면 똑같이 구불구불 한 모양인 것을 알 수 있따.  
그럼 여기서 `local minimum`과 `global minimum`은 뭘까? 

- local minimum : 한 구간에서의 최솟점 
- global minumum : 모든 구간에서의 최솟점 

이라고 생각하면 될 것 같다.

따라서 위 문제점은 gradient descent을 사용할 때 $cost$를 작게 만드는 $W$를 찾을 수 없다.  

그럼 어떻게 해야할까? 

위에서 가설을 바꿨기 때문에 똑같이 cost도 바꿔주어야 한다. 

<br>

### 어떻게 바꿔? 
cost를 아래와 같이 바꾸면 된다.

<img src="/assets/img/Modeep1/new cost.png" width="70%" height="70%">

일단 $cost$가 $W$에 대한 함수이고, 모든 값을 다 더한뒤 평균을 내는 것이기 때문에  
$cost(W) = \frac{1}{m} \sum$ 부분은 가만히 둔다.  

그리고 하나의 엘리먼트에 대한 cost에 대한 값을 $c$ 함수라고 정의한다.   
$c$ 함수는 두 가지 케이스로 나누어서 함수를 정의하게 된다.  

- y = 1 
    - $-log(H(x))$

- y = 0
    - $-log(1 - H(x))$

그럼 왜 이렇게 하고 log를 사용할까?

위 가설

$$\frac{1}{1 + e^{-z}}$$ 

여기에서 $e^{-z}$ 부분이 꾸불하게 만든다.  
그런데 $e^{-z}$ 은 `log와 상극`이기 때문에 꾸불함을 잡아주는 이유와 그래프를 그려 봤을 때 

<img src="/assets/img/Modeep1/-log(x).png" width="50%" height="50%">

이렇게 나타나는데 경사 하강법을 할 때의 그래프와 비슷하기 때문이다.
<br><br>

## 그럼 원리는? 

<img src="/assets/img/Modeep1/new cost.png" width="70%" height="70%">

이 그림에서 조건은 총 2개 였다. 

- y = 1 
    - $-log(H(x))$

- y = 0
    - $-log(1 - H(x))$

일단 $cost$는 정답이 맞다면 값을 작게하고 틀리면 값을 크게 준다는 것을 잊지 말자.

<br>

### y = 1 일 때

- $-log(H(x))$

$-log(x)$를 그림으로 그려보면 

<img src="/assets/img/Modeep1/-log(x).png" width="50%" height="50%">

와 같고 위에서 $cost$ 값은 정답이 맞다면 작게, 틀리면 크게라고 했다.  

$H(x) = 1$ -> $cost(1)$ 이렇게 되면 0에 가깝게 된다.

<img src="/assets/img/Modeep1/y=1 cost=1.png" width="50%" height="50%">

따라서 $H(x) = 1$ -> $cost(1) = 0$

하지만 반대로 $H(x) = 0$ -> $cost(1)$ 이라면 값이 무한에 가까워진다.

이런 원리로 cost를 구할 수 있다.

<br>

### y = 0 일 때

y = 1 반대를 생각하면 된다.

<img src="/assets/img/Modeep1/-log(x) 반대.png" width="50%" height="50%">

그러면 정답이 0일 때 가설이 0이면 $H(x) = 0$ -> $cost(0) = 0$이 되고  
가설이 1이면 $H(x) = 0$ -> $cost(1)$ 은 무한에 가까워진다.

<br>

### 그럼 두 개를 붙이면? 

자 그럼 위 두 그래프를 서로 합치면 아래로 볼록한 그래프가 되면서 경사 하강법을 할 수 있는 그래프가 만들어 진다.  
또한 그래프를 합치는 것만 아니라 식을 두 개로 합쳐서 간단하게 만들 수 있다. 

$$c: H((x),y) = -ylog(H(x)) - (1 - y)log(1 - H(x))$$

이게 뭐야? 라고 할 수 있지만 정말 간단하다.  

이 부분 1이 들어간다면 뒷 부분 $(1 - y)log(1 - H(x))$ 이 사라지게 되면서 앞 부분 $-ylog(H(x))$ 만 계산할 수 있고,  
0이 들어간다면 첫 번째 부분이 사라지면서 $(1 - y)log(1 - H(x))$ 이 부분만 계산할 수 있다.  


<br>

### 마지막으로..
이제 마지막으로 위 cost를 구했으니 이제 경사 하강법은 아래와 같이 나타낸다.

$$H((x),y) = -ylog(H(x)) - (1 - y)log(1 - H(x))$$

$$cost(W) = -\frac{1}{n} \sum_{i}^{n} ylog(H(x)) - (1 - y)log(1 - H(x))$$

$$W = W - \alpha\frac{∂}{∂W} cost(W)$$

미분한 값을 계산하려면 힘들지만 인터넷에 좋은 사이트도 많고, 프로그램도 있고 하니 직접 계산하진 말자..  


<br><br>