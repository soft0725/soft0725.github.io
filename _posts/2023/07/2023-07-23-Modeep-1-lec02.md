---
title: 모딥 시즌 1 - ML lec 02
categories: [DeepLearning, 모두를 위한 딥러닝 시즌 1]
tags: [모두를 위한 딥러닝 시즌 1]
math: true
---

# lec 02 정리 
## 목차 


## Linear regression

| X(hour) | Y(score)| 
|:---:|:---:|
|10|90|
|9|80|
|3|50|
|2|30|

위 데이터는 공부 시간에 따른 시험 점수를 기록한 training data라고 가정하자.  
그럼 위 데이터로 regression 모델을 학습 시키면 모델에게 X라는 값을 주었을 때 모델이 예측한 값 Y를 돌려받게 된다.  
예를 들어 7시간을 공부했다고 하면 65점이나 70점 사이로 예측할 수 있을 것이며 이것을   
`Linear regression 선형 회귀` 라고 부른다.  
선형회귀는 임의의 선을 하나 그어 데이터를 예측한다고 생각하면 된다.  
아래서 더 자세히 설명하겠다.  

<br>

### Linear regression의 동작 방법
설명을 하기 위해서 training data를 하나 만들어보겠다.

| X | Y| 
|:---:|:---:|
|1|1|
|2|2|
|3|3|

데이터가 위처럼 있다고 가정하고, 위 데이터를 시각화하면 아래처럼 된다.
<img src="/assets/img/Modeep1/Linear regression data.png" width="35%" height="35%">

그리고 위 데이터를 하나의 직선으로 예측을 하기 위해선 Hypothesis 가설을 세워야 한다.  

<br>

### Hypothesis
Hypothesis는 어떤 방식으로 세워야 할까?  
일단 우리는 Linear한 모델을 학습시키기 위한 가설을 찾는 것 이므로 인간의 입장에서 가설을 세워본다면 아래와 같을 것이다.  

<img src="/assets/img/Modeep1/Linear regressioin data draw line.png" width="35%" height="35%"> 

위 가설이 1차 방정식이라는 것을 알 수 있지만 어떻게 구해야할까?   
인간의 입장에서는 선을 점에 대해서 최대한 가깝게 긋기위해서 노력한다.

위에서 한가지 힌트가 보인다. `점들에 대해서 최대한 가깝게 그리도록 노력한다.`  
그럼 위 원리와 1차 방정식의 원리를 사용하여 가설을 정의해보자 -> $H(x) = Wx + b$  
$H(x)$는 가설이 될 것이고 $W, b$에 따라서 가설이 달라질 것이다.  
수학시간에 배웠던 1차 방정식의 기울기와 y절편 원리와 똑같다.

<br>

### Which better? 
그럼 어떤 가설이 제일 좋을까?  
위에서 말했던 최대한 가깝게라는 말을 생각해보면 된다.  
최대한 가깝다 -> 선과 점에 대한 거리가 가깝다   
그 원리를 적용하면 아래와 같다.    

#### 그림 1.1
<img src="/assets/img/Modeep1/Hypothesis_data.png" width="40%" height="40%">    

파란색 선을 가설, x표시를 실제 데이터라고 가정하자.  
그럼 가설과 실제 데이터까지의 거리를 빨간색 선으로 표시할 수 있다.

<br>

### Cost function 
위 빨간색 선을 Regression에서는 Cost function 비용함수라고 부른다.  
또 다른 뜻으로 Loss function이라고도 부른다.  
그럼 인터넷에서 Cost, Loss function이 있다면 그 모델은 Linear한 모델인 것을 알 수 있고, 가설과 실제 데이터가 얼마나 차이가 나는지 알 수 있다.  

그럼 빨간색선 Cost function은 어떻게 구할까?  

1. `가설과 실제 데이터의 y값을 빼서 구할 수 있다.`  
$H(x) - y$  
하지만 이렇게 되면 실제 대이터가 음수 일때는 덧셈이 되어버리니 좋진않다.  


2. `가설과 실제 데이터의 y값을 빼서 제곱한다.`  
$(H(x) - y)^{2}$ 
데이터와 가설이 멀다면 제곱을 하기에 더 큰 패널티를 줄 수 있다는 장점이 있으며  
만약 y가 음수라도 제곱을 하기에 플러스로 만들어 줄 수 있다.  
그래서 이것을 가장많이 사용하고 Cost function이라고 부른다.

그럼 Cost function을 그림 1.1에 대해서 구해보자  
  
$$\frac{(H(x(1)) - y(1))^{2} + (H(x(2)) - y(1))^{2} + (H(x(3)) - y(3))^{2}}{3}$$

결국엔 가설과 실제 점의 거리를 구하는 것이기에 다 더하고 데이터의 갯수만큼 나눠준다면  
가설과 데이터까지 차이의 평균을 구하게 된다.  
그럼 위 식을 더 간단하게 고치면 아래와 같다.  

$$\frac{1}{n} \sum_{i=1}^{n} (H(x_i) - y_i)^2$$

혹시나 위 식을 모르는 사람이 있다면  
- $\frac{1}{n}$ 은 데이터의 갯수만큼 나눠준다.  

- $\sum_{i=1}^{n}$ 은 1부터 n까지의 수를 다 더한다  

- $(H(x_i) - y_i)^2$ 위 식 i가 이 안에 들어간다고 생각하면 된다.  

- 그래서 결국에는 i부터 n까지의 가설과 데이터 차에 제곱을 해서 다 더하고 데이터의 갯수만큼 나눠준다 라고 생각하면 된다.  

이제 마지막으로 $H(x) = Wx + b$ 라는 식을 $Cost = \frac{1}{n} \sum_{i=1}^{n} (H(x_i) - y_i)^2$에 대입하면   

$$Cost(W, b) = \frac{1}{n} \sum_{i=1}^{n} (H(x_i) - y_i)^2$$  

$W, b$에 대한 function이 된다.  
또한 이것을 간단하게 말하면 아래와 같다.
<img src="/assets/img/Modeep1/minimize.png" width="40%" height="40%">  
~~마크다운으로 표현이 불가능하여 사진으로 대체~~  

$Cost$가 $W, b$에 대한 function이고 이를 가장 작게하는 값 $W, b$를 구한다는 뜻이 되겠다.   
-> $Cost$ 값을 최소화 하는 값 $W, b$를 구한다.

<br>

### 이제 이것을 정리해보자면  
1. 가설을 세울 때는 가설과 실제 데이터의 차를 제곱을 해서 다 더한뒤 평균을 구한다.  

2. 위 방법을 Cost function 이라고 하며 MSE와 같은 방법들이 있다.  

3. 그리고 최종 목표는 Cost 값을 최소화하는 값 $W, b$를 구하는 것이 목표이다.  


다음 블로그에는 Deeplearning 카테고리 안에 원리라는 이름으로  
어떤 Cost function이 있는지와 구현을 해보는 글을 올리도록 하겠다.  
