---
title: PyTorch 기초 - 여러 함수들 
date: '2023-08-05 14:12:00 +0900'
categories: [DeepLearning, PyTorch]
tags: [PyTorch]
math: true
---

## PyTorch의 여러 함수들 

```python
A=torch.randn(3,3) # Normal 의 n
B=torch.rand(3,3) # 이건 uniform
print(A)
print(B)
print(A[A[:,0]<0,:]) # 0 번째 열이 음수인 행들     
```
```
tensor([[-2.2280, -0.2655,  0.5285],
        [-1.5944,  0.0820, -0.5476],
        [ 0.5724,  0.3434, -0.5678]])

tensor([[0.5707, 0.2209, 0.7139],
        [0.3488, 0.2078, 0.7612],
        [0.6228, 0.7249, 0.1137]])

tensor([[-2.2280, -0.2655,  0.5285],
        [-1.5944,  0.0820, -0.5476]])
```

위 값은 실행할 때마다 달라진다는 것을 잊지말자   

### randn vs rand  
randn은 정규분포를 활용하여 난수를 생성하는 함수이다.  
따라서 평균값은 0이 되고 여기서 정규분포를 설명하진 않는다.  

그리고 rand는 0과 1사이의 난수를 생성하는 함수이다. 


<br>


```python 
A=torch.randn(3,3)
print(A)
print(torch.abs(A)) # 절댓값 
print(torch.sqrt(torch.abs(A))) # 루트 
print(torch.exp(A)) # 자연상수 e 
print(torch.log(torch.abs(A)))
print(torch.log(torch.exp(torch.tensor(1)))) # torch.exp(torch.tensor(1)) = e^1
print(torch.log10(torch.tensor(10)))
print(torch.log2(torch.tensor(2)))
print(torch.round(A)) # 반올림
print(torch.round(A, decimals=2)) # 소수점 둘째자리까지
print(torch.floor(A)) # 내림
print(torch.ceil(A)) # 올림
```
```
tensor([[ 0.0410, -1.1713, -0.1850],
        [-0.3433, -0.9131, -0.4786],
        [ 0.5371,  0.3436, -1.4821]])

tensor([[0.0410, 1.1713, 0.1850],
        [0.3433, 0.9131, 0.4786],
        [0.5371, 0.3436, 1.4821]])

tensor([[0.2026, 1.0823, 0.4301],
        [0.5859, 0.9556, 0.6918],
        [0.7329, 0.5862, 1.2174]])

tensor([[1.0419, 0.3100, 0.8311],
        [0.7094, 0.4013, 0.6197],
        [1.7111, 1.4100, 0.2272]])

tensor([[-3.1932,  0.1581, -1.6876],
        [-1.0691, -0.0909, -0.7369],
        [-0.6216, -1.0683,  0.3935]])

tensor(1.)

tensor(1.)

tensor(1.)

tensor([[ 0., -1., -0.],
        [-0., -1., -0.],
        [ 1.,  0., -1.]])

tensor([[ 0.0400, -1.1700, -0.1800],
        [-0.3400, -0.9100, -0.4800],
        [ 0.5400,  0.3400, -1.4800]])

tensor([[ 0., -2., -1.],
        [-1., -1., -1.],
        [ 0.,  0., -2.]])

tensor([[ 1., -1., -0.],
        [-0., -0., -0.],
        [ 1.,  1., -1.]])
```
위 출력이 어떻게 되는지 나도 모르는것이 몇가지 있다.  
그 부분은 내가 따로 공부해서 채워보도록 하겠다.  

log10, log2는 사람들이 많이 써서 정의되어 있는 함수이다.  
따라서 log3과 같은 함수는 존재하지 않으며 사용시 따로 만들어주어야한다.  

<br>

```python 
print(torch.sin(torch.tensor(torch.pi/6))) # type(torch.pi) <- float임 만약 tensor와 연산하면 tensor로 바꿔줌
print(torch.cos(torch.tensor(torch.pi/3)))
print(torch.tan(torch.tensor(torch.pi/4)))
print(torch.tanh(torch.tensor(-10)))

print(type(torch.pi / 6))
print(type(torch.pi / torch.tensor(6)))
```
```
tensor(0.5000)
tensor(0.5000)
tensor(1.)
tensor(-1.)
<class 'float'>
<class 'torch.Tensor'>
```

기본적으로 torch.pi는 3.141592... 의 float값을 가진다.  
따라서 float형태인 torch.pi를 6과 같은 실수나 정수로 계산하면 type은 float이 되고,  
반대로 float의 형태에서 torch.tensor과 연산하면 torch.tensor의 타입으로 바뀐다.  


<br>

```python
torch.nan # not a number
print(torch.log(torch.tensor(-1)))
print(torch.isnan(torch.tensor([1,2,torch.nan,3,4])))
print(torch.isinf(torch.tensor([1,2,3,4,torch.inf])))
```
```
tensor(nan)
tensor([False, False,  True, False, False])
tensor([False, False, False, False,  True])
```

nan은 숫자가 아니라는 뜻이다.  
그리고 torch.isnan은 값이 없는게 있냐? 라는 뜻이 되며  
아래있는 torch.isinf는 무한대인 숫자가 있냐고 물어보는 뜻이다.  

<br>

```python 
A=torch.randn(3,4)
print(A)
print(); print()
print(torch.max(A,dim=0))
print(); print()
print(torch.max(A,dim=1))
```
```
tensor([[ 0.7951,  0.3772, -0.5206, -0.2561],
        [ 0.4562,  0.0461, -2.1190,  1.3717],
        [-0.4210,  0.7648,  1.9744,  0.7007]])


torch.return_types.max(
values=tensor([0.7951, 0.7648, 1.9744, 1.3717]),
indices=tensor([0, 2, 2, 1]))


torch.return_types.max(
values=tensor([0.7951, 1.3717, 1.9744]),
indices=tensor([0, 3, 2]))
```

dim=0이라는 것은 열을 의미한다.  
따라서 dim=0인 경우 위에서 아래로 [0.7951, 0.4562, -0.4210] 이 숫자들에 대해서 가장 큰 수를 비교한다.  
반대로 dim=1이라는 것은 행을 의미하며 이번에는 왼쪽에서 오른쪽으로 비교한다.    
또한 values아래는 indices라는게 있는데 이건 그 행, 열에서의 인덱스 번호를 저장해놓는다.  

하지만 문제점이 있다.  
나는 분명 3행 4열짜리를 선언했는데 출력값은 1차원이 되어버린다는 것이다.  
2차원 -> 1차원으로 되었으니 이것을 유지하는 방법이 없을까?  

```python 
print(torch.max(A,dim=0, keepdims=True))
print(torch.max(A,dim=1, keepdims=True)) # 3 행 1열 짜리 2D tensor
```
```
torch.return_types.max(
values=tensor([[0.2130, 1.2912, 0.8666, 0.3396]]),
indices=tensor([[2, 1, 0, 0]]))
torch.return_types.max(
values=tensor([[0.8666],
        [1.2912],
        [0.2208]]),
indices=tensor([[2],
        [1],
        [2]]))
```

위의 결과와는 다르게 2차원을 유지하면서 계산한 모습을 확인할 수 있다.  
확인하는 방법은 [ 이것의 갯수가 처음 몇 개 있는지 보면 된다.  

<br>

```python 
A=torch.randn(3,4)
print(A)
print(torch.argmax(A,dim=0)) # 각 열에서 가장 큰 애가 존재하는 인덱스
```
```
tensor([[ 0.7429, -0.7646, -0.3159, -1.3180],
        [ 0.8416, -0.9856, -1.4308,  0.7283],
        [ 0.8008,  1.2337, -1.1175,  0.7386]])
tensor([1, 2, 0, 2])
```

argmax는 각 행, 열에서 가장 큰 애가 존재하는 인덱스 번호를 알려준다.  


<br>

```python 
a=torch.randn(6,1)
print(a); print()

a_sorted=torch.sort(a,dim=0)
print(a_sorted)
```
```
tensor([[ 0.3110],
        [-0.2442],
        [ 1.3310],
        [ 0.8365],
        [ 0.7332],
        [-0.7442]])

torch.return_types.sort(
values=tensor([[-0.7442],
        [-0.2442],
        [ 0.3110],
        [ 0.7332],
        [ 0.8365],
        [ 1.3310]]),
indices=tensor([[5],
        [1],
        [0],
        [4],
        [3],
        [2]]))
```

torch.sort(a, dim=0)은 열에 대해서 정렬을 해준다.  
그리고 마지막에 각각 정렬되면서 인덱스는 어떻게 바뀌었는지 출력해준다.  

반대로 dim=1을 해버리면 

```
torch.return_types.sort(
values=tensor([[-0.3376],
        [-0.6926],
        [-0.6219],
        [ 0.9121],
        [-1.6039],
        [-0.7826]]),
indices=tensor([[0],
        [0],
        [0],
        [0],
        [0],
        [0]]))
```

이런식으로 정렬을 잘 하지 못한다.  
가장 큰 이유는 6,1 이기 떄문에 행이 1개라는 점에서 정렬을 잘 하지 못한다.  
만약 행이 여러개가 있다면 정렬을 잘 하겠지만 1개 있을때 정렬하면 애가 바보가 되어버린다.  

그리고 만약 정렬을 오름차순에서 내림차순으로 하고 싶다면 뒤에 ,descending=True 를 넣어주면 된다.

<br>

```python
A=torch.randn(3,4)
print(A)
print(torch.sum(A))
print(torch.mean(A))
print(torch.std(A)) # standard deviation 표준 편차
```
```
tensor([[-0.5003,  0.1958,  0.2131,  0.3091],
        [-0.4984,  1.7362, -0.2464,  0.9270],
        [ 0.0987, -0.5086, -0.3137,  0.7611]])
tensor(2.1735)
tensor(0.1811)
tensor(0.6852)
```

이런 sum, mean, std같은 함수가 있고 뒤에 dim=1, dim=0, keepdims=True, 등등 똑같이 상황에 따라서 써주면 된다.  

<br>

```python
A=torch.randint(1,5,size=(12,)) # 1부터 5미만 12개 정수 (1 차원은 (N,) 과 같이 표현)
print(A)
print(A.shape)

B=A.reshape(2,2,3)
print(B)
print(B.ndim) # 3 차원 행렬이다
```
```
tensor([2, 3, 2, 1, 3, 2, 2, 1, 1, 2, 3, 2])
torch.Size([12])
tensor([[[2, 3, 2],
         [1, 3, 2]],

        [[2, 1, 1],
         [2, 3, 2]]])
3
```
맨 처음 1차원 A를 선언했다.  
그리고 A를 reshape한 결과를 B에 담아주었는데 이때의 결과는 3행 2열 2개 이다.  
따라서 3차원이 된다.  

<br>

```python
a=torch.tensor([1,2,3])
b=torch.tensor([2,2,1])
print(torch.sum(a*b))

a=a.reshape(3,1)
b=b.reshape(3,1)
# 트랜스포즈 종류들 
print(a.transpose(1,0)@b)
print(a.permute(1,0)@b)
print(a.T@b)
print(a.t()@b)

A=torch.randn(4,3,6)
print(A.permute(0,2,1).shape)
print(A.transpose(0,2).shape) # transpose 는 둘끼리 자리 바꾸기만 가능
```

처음 a*b는 그냥 곱셈이다.  
이제 아래를 보면 행렬곱을 하게 되는데 행렬곱을 할때의 조건은 [1,3][3,2] 처럼 중간의 숫자가 서로 같아야 한다.  
만약 위 [1,3][3,2] 이 크기의 행렬을 계산해보면 1,2가 나온다.  
그렇기 때문에 지금 [3,1][3,1] 이라서 행렬곱을 할 수 없는데 아래에서 트랜스포즈를 취한뒤 행렬곱을 하는 예시이다.  

[3,1][3,1]중 앞을 트랜스포즈를 취하면 [1,3][3,1]로 되면서 계산이 된다.  
이때 트랜스포즈를 해주는 함수는 총 4개가 있다.  
a.t(), a.T, a.permute(1,0), a.transpose(1,0)  

permute와 transpose의 차이는 permutes는 여러개의 숫자를 바꾸면서 차원과 행열 모두 바꿀 수 있는데  
transpose는 두개밖에 바꾸지 못한다는 단점이 있다.  
그래서 permute를 많이 쓴다고 한다.  

<br>

```python
A=torch.arange(20)
print(A)
print(A.reshape(4,5))
print(A.reshape(4,-1).shape) # 4개 행이 될 수 있도록 열의 수를 맞춰라
```
```
tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
        18, 19])
tensor([[ 0,  1,  2,  3,  4],
        [ 5,  6,  7,  8,  9],
        [10, 11, 12, 13, 14],
        [15, 16, 17, 18, 19]])
torch.Size([4, 5])
```
reshape을 할 때 -1을 하면 알아서 채워 넣어달라는 뜻이된다.  
만약 20, 이라면 1차원이라는 뜻이 되는데 reshape(-1, 4)를 하면 -1 부분에는 5가 자동으로 채워지게 된다.  
그리고 -1, -1을 해주면 에러가 나기때문에 하나만 적어주도록 한다.  

<br>

```python 
x=torch.randn(2,3,4,5,6)
print(x[1,2,:,:,:].shape) 
print(x[1,2,...].shape) # x[1, 2, …] 는 x[1, 2, :, :, :] 와 같습니다.
print(x[:,:,:,:,3].shape)
print(x[...,3].shape) # x[…, 3] 는 x[:, :, :, :, 3] 와 같습니다.
print(x[1,:,:,3,:].shape)
print(x[1,...,3,:].shape) # x[1, …, 3, :] 는 x[1, :, :, 3, :] 와 같습니다.
```
```
torch.Size([4, 5, 6])
torch.Size([4, 5, 6])
torch.Size([2, 3, 4, 5])
torch.Size([2, 3, 4, 5])
torch.Size([3, 4, 6])
torch.Size([3, 4, 6])
```

솔직히 나도 이런건 처음봐서 어색하고 잘 모르겠다.  
그래서 이 부분은 조금 더 봐야 알 수 있을것 같은데 ...을 하면 자동으로 중간 :을 채워주는 것 같다.  

<br>

```python
A=torch.ones((2,3,4))
B=torch.zeros((2,3,4))

C=torch.vstack([A,B])
D=torch.hstack([A,B]) # v는 0번째 차원, h는 1번째 차원에 쌓는다.

print(C)
print(D)
```
```
tensor([[[1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.]],

        [[1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]]])
tensor([[[1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]]])
```
vstack는 0번째 차원, hstack는 1번째 차원에 쌓는다.  
그리고 이와 비슷한 cat이 있는데 cat이 더 보기가 쉽다.

```python
A=torch.ones((2,3,4))
B=torch.zeros((2,3,4))

E=torch.cat([A,B], dim=0)
F=torch.cat([A,B], dim=1)
G=torch.cat([A,B], dim=2)

print(E)
print(F)
print(G)
```
```
tensor([[[1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.]],

        [[1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]]])
tensor([[[1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]]])
tensor([[[1., 1., 1., 1., 0., 0., 0., 0.],
         [1., 1., 1., 1., 0., 0., 0., 0.],
         [1., 1., 1., 1., 0., 0., 0., 0.]],

        [[1., 1., 1., 1., 0., 0., 0., 0.],
         [1., 1., 1., 1., 0., 0., 0., 0.],
         [1., 1., 1., 1., 0., 0., 0., 0.]]])
```
dim을 사용하는 방식이라서 훨씬 알기 쉽고 편하다.  

<br>

```python 
A = torch.randn(1,1,1,3,1,1,4,1)
# print(A)
print(A.shape)
print(A.squeeze().shape)
```
```
torch.Size([1, 1, 1, 3, 1, 1, 4, 1])
torch.Size([3, 4])
```

shape을 하면 적었던 차원 1포함해서 전부 나온다.  
하지만 squeeze를 하면 불필요한 1이라는 것을 다 없애버리고 3, 4만 남게 된다.  


<br>

```python 
A = torch.randn(3,4)
print(A.unsqueeze(dim=0).shape)
print(A.unsqueeze(dim=1).shape)
print(A.unsqueeze(dim=2).shape)
print(A.reshape(1,3,4).shape)
print(A.reshape(3,1,4).shape)
print(A.reshape(3,4,1).shape)
```
```
torch.Size([1, 3, 4])
torch.Size([3, 1, 4])
torch.Size([3, 4, 1])
torch.Size([1, 3, 4])
torch.Size([3, 1, 4])
torch.Size([3, 4, 1])
```

unsqueeze는 squeeze의 반대라고 생각하면 된다.  
squeeze는 차원을 축소하지만 unsqueeze는 차원을 늘려준다.  
따라서 맨 처음 dim=0에 대해서 unsqueeze를 하면 1, 3, 4가 되며 1에서는 3, 1, 4  
2에서는 3, 4, 1이 된다.  

그리고 reshape은 그냥 자신이 원하는만큼 지정해주면 된다.  

<br>

```python 
A=torch.ones(3,4)
B=torch.zeros(3,4)
A=A.unsqueeze(dim=0)
B=B.unsqueeze(dim=0)
C=torch.cat([A,B], dim=0)
print(C)
print(C.shape)
```
```
tensor([[[1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]]])
torch.Size([2, 3, 4])
```
만약 내가 차원을 늘리고 cat을 하고 싶은데 그러지 못하는 상황이라면 unsqueeze를 dim=0에 대해서 한 뒤   
cat을 해버리면 차원에 대해서 서로 붙일 수 있다.  


<br>

```python
A=torch.tensor([[1,2],[3,4]])
B=A.clone()
B[0,0]=100

print(B)
print(A)
```
```
tensor([[100,   2],
        [  3,   4]])
tensor([[1, 2],
        [3, 4]])

```

B = A라고 해버리면 id를 공유하면서 B를 바꾸면 A도 같이 바뀌게 된다.  
그떄 clone을 사용하면 B는 다른 id를 사용하기 떄문에 B를 바꿔도 A가 바뀌지 않는다.  

<br>

## 행렬곱에 대해서 

```python 
A=torch.randn(32,5,7)
B=torch.randn(32,7,10)
C=A@B
print(C.shape)
```
```
torch.Size([32, 5, 10])
```
처음 행렬곱을 했을 때 [3,1][1,3]이면 [3,3]이 나왔는데 앞에 하나가 더 있다면 어떻게 해야할까  
그냥 앞은 그대로 나두고 뒤에만 해주면 된다.  

<br>

```python 
A=torch.randn(32,5,7)
B=torch.randn(7,10)

C=A@B.repeat(32,1,1)
D=A@B

print(C.shape)
print(D.shape)
```
```
torch.Size([32, 5, 10])
torch.Size([32, 5, 10])
```

음 이해는 잘 안되는데 차원을 늘리는 것 같다.  
아 찾아보니 반복하는거라고 한다.  
그 예시는 아래있다.  

```python 
A = torch.rand(2,3)
A_repeat=A.repeat(3,1,3,2)
print(A)
print(A_repeat)
print(A_repeat.shape)
```
```
tensor([[0.8109, 0.0390, 0.5317],
        [0.2592, 0.0011, 0.0123]])
tensor([[[[0.8109, 0.0390, 0.5317, 0.8109, 0.0390, 0.5317],
          [0.2592, 0.0011, 0.0123, 0.2592, 0.0011, 0.0123],
          [0.8109, 0.0390, 0.5317, 0.8109, 0.0390, 0.5317],
          [0.2592, 0.0011, 0.0123, 0.2592, 0.0011, 0.0123],
          [0.8109, 0.0390, 0.5317, 0.8109, 0.0390, 0.5317],
          [0.2592, 0.0011, 0.0123, 0.2592, 0.0011, 0.0123]]],


        [[[0.8109, 0.0390, 0.5317, 0.8109, 0.0390, 0.5317],
          [0.2592, 0.0011, 0.0123, 0.2592, 0.0011, 0.0123],
          [0.8109, 0.0390, 0.5317, 0.8109, 0.0390, 0.5317],
          [0.2592, 0.0011, 0.0123, 0.2592, 0.0011, 0.0123],
          [0.8109, 0.0390, 0.5317, 0.8109, 0.0390, 0.5317],
          [0.2592, 0.0011, 0.0123, 0.2592, 0.0011, 0.0123]]],


        [[[0.8109, 0.0390, 0.5317, 0.8109, 0.0390, 0.5317],
          [0.2592, 0.0011, 0.0123, 0.2592, 0.0011, 0.0123],
          [0.8109, 0.0390, 0.5317, 0.8109, 0.0390, 0.5317],
          [0.2592, 0.0011, 0.0123, 0.2592, 0.0011, 0.0123],
          [0.8109, 0.0390, 0.5317, 0.8109, 0.0390, 0.5317],
          [0.2592, 0.0011, 0.0123, 0.2592, 0.0011, 0.0123]]]])
torch.Size([3, 1, 6, 6])
```
이렇게 여러개를 반복하는? 그런것 같다.  



